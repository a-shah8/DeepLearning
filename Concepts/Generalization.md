## **Generalization**

The ability to perform well on previously unobserved inputs is called ***generalization***.

We want not just the training error to be low, but **generalization error**, also called the **test error**, to be low as well.

#### Now, how can this be done?

    The field of 'Statistical Learning Theory' provides some answers, given we are allowed to make some assumptions
    about how the training and test set are collected.

I recommend the readers to google 'Generalization' to gain in-depth knowledge and intuition about how generalization can be achieved while training a learning model.
